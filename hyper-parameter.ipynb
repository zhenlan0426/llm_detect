{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import nlpaug.augmenter.word as naw\n",
    "from functools import partial\n",
    "from functions import *\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 16\n",
    "accumulation_steps = 64\n",
    "verbose = 2048\n",
    "lr = 8e-5\n",
    "clip = 8e-3\n",
    "\n",
    "save_model_path = get_next_folder_name('Model/')\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.makedirs(save_model_path)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get random hyper-parameters ##\n",
    "TARGET_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "pred_type = np.random.choice(['LM','classification'])\n",
    "config_type = np.random.choice(['prefix','prompt_encoder','prompt_txt','LoRA'],p=[0.3,0.3,0.1,0.3])\n",
    "epochs = 2\n",
    "alpha = np.random.rand()*0.1\n",
    "aug_kwargs = dict(aug_max=np.random.randint(5,30),aug_p=np.random.rand()*0.3)\n",
    "config_class,config_kwargs = get_random_config(config_type,pred_type,TARGET_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mistralai/Mistral-7B-v0.1',\n",
       " 'LM',\n",
       " 'prompt_encoder',\n",
       " 2,\n",
       " 0.01188804794330508,\n",
       " {'aug_max': 25, 'aug_p': 0.23551110723313648},\n",
       " peft.tuners.p_tuning.config.PromptEncoderConfig,\n",
       " {'task_type': 'CAUSAL_LM',\n",
       "  'num_virtual_tokens': 32,\n",
       "  'encoder_hidden_size': 2048,\n",
       "  'encoder_dropout': 0.0815198204570457,\n",
       "  'encoder_num_layers': 3,\n",
       "  'encoder_reparameterization_type': 'MLP'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_MODEL, pred_type, config_type, epochs, alpha, aug_kwargs, config_class, config_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hyper-parameter for inference\n",
    "# with open(save_model_path+'/config.pkl', 'rb') as pickle_file:\n",
    "#     config = pickle.load(pickle_file)\n",
    "# TARGET_MODEL, pred_type, config_type, epochs, alpha, aug_kwargs, config_class, config_kwargs = load_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('data/val_data2.csv')\n",
    "train = pd.read_csv('data/train_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://paraphrase.org/#/download\n",
    "# https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb\n",
    "aug = naw.SynonymAug(aug_src='ppdb',model_path='Model/ppdb-2.0-s-all',\n",
    "                     aug_min=1,\n",
    "                     **aug_kwargs)\n",
    "# print(aug.augment(train.text.iloc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred_type == 'LM':\n",
    "    prompt = tokenizer.batch_encode_plus(['Is this essay AI-generated, yes or no?'],add_special_tokens=False,return_tensors='pt')\n",
    "    prompt,prompt_mask = prompt['input_ids'],prompt['attention_mask']\n",
    "else:\n",
    "    prompt,prompt_mask = None, None\n",
    "train_data = TxtData(train,aug.augment)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, \\\n",
    "                        collate_fn=partial(collate_fn,tokenizer=tokenizer,prompt=prompt,prompt_mask=prompt_mask))\n",
    "val_data = InfData(val)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size*2, shuffle=False, num_workers=num_workers, \\\n",
    "                        collate_fn=partial(collate_inf,tokenizer=tokenizer,prompt=prompt,prompt_mask=prompt_mask))\n",
    "# input_ids,attention_mask, label, score, topic = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change hidden_states output in modeling_mistral.py\n",
    "# return CausalLMOutputWithPast(\n",
    "#     loss=loss,\n",
    "#     logits=logits,\n",
    "#     past_key_values=outputs.past_key_values,\n",
    "#     hidden_states=outputs.hidden_states,\n",
    "#     hidden_states=hidden_states,\n",
    "#     attentions=outputs.attentions,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af77fe795bcb49168b0f96ce2655fc39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenlan/anaconda3/lib/python3.10/site-packages/peft/tuners/p_tuning/model.py:106: UserWarning: for MLP, the argument `encoder_num_layers` is ignored. Exactly 2 MLP layers are used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_value_\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_model\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "base_class = AutoModelForCausalLM if pred_type == 'LM' else AutoModelForSequenceClassification\n",
    "base_model = base_class.from_pretrained(TARGET_MODEL,quantization_config=nf4_config, \\\n",
    "                                                          device_map={\"\":0},use_flash_attention_2=True)\n",
    "# base_model.config.output_hidden_states = True\n",
    "peft_config = config_class(**config_kwargs)    \n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.print_trainable_parameters()\n",
    "topicModel = MLP(model.config.hidden_size,14,2,2048,0.1).to('cuda')\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad] + [param for param in topicModel.parameters()]\n",
    "trainable_names = [name for name,param in model.named_parameters() if param.requires_grad]\n",
    "# optimizer = torch.optim.AdamW(trainable_params,lr = lr,amsgrad=True,weight_decay=6e-3)\n",
    "optimizer = torch.optim.SGD(trainable_params,lr=lr)\n",
    "if pred_type == 'LM':\n",
    "    model_lm = LM(model,tokenizer,config_kwargs.get('num_virtual_tokens', 0),alpha,config_type,topicModel,beta_)\n",
    "else:\n",
    "    model_lm = Classification(model,alpha,topicModel,beta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 2047: train loss [0.63553728 5.80410825 2.62093398], test AUC 0.800629192438336\n",
      "epoch 0 iter 4095: train loss [0.47754546 5.85188775 2.55820129], test AUC 0.867756586335894\n",
      "epoch 0 iter 6143: train loss [0.38865272 5.89569744 2.50375905], test AUC 0.9106689490808997\n",
      "epoch 0 iter 8191: train loss [0.3506362  5.81816955 2.47734403], test AUC 0.8952515302643023\n",
      "epoch 0 iter 10239: train loss [0.32225891 5.85346146 2.43175036], test AUC 0.9047633182568983\n",
      "epoch 0 iter 12287: train loss [0.29128328 5.89913991 2.40395433], test AUC 0.9415989782907251\n",
      "epoch 0 iter 14335: train loss [0.29977784 5.88557755 2.37738738], test AUC 0.9454852511740635\n",
      "epoch 0 iter 16383: train loss [0.27169858 5.91630668 2.37869804], test AUC 0.9502326154591939\n",
      "epoch 0 iter 18431: train loss [0.24565269 5.89975199 2.312086  ], test AUC 0.9561679296780586\n",
      "epoch 0 iter 20479: train loss [0.22636379 5.95649252 2.31952977], test AUC 0.9483599003129244\n",
      "epoch 0 iter 22527: train loss [0.20791936 5.99428955 2.31968493], test AUC 0.956392841102126\n",
      "epoch 0 iter 24575: train loss [0.20909265 6.07660275 2.30217038], test AUC 0.9549294838542122\n",
      "epoch 0 iter 26623: train loss [0.20810241 6.11874126 2.28784059], test AUC 0.964425645415949\n",
      "epoch 0 iter 28671: train loss [0.19856554 6.14714778 2.28703874], test AUC 0.9556501420913406\n",
      "epoch 0 iter 30719: train loss [0.19567699 6.1667041  2.25036743], test AUC 0.9697729554657639\n",
      "epoch 0 iter 32767: train loss [0.19266561 6.14847665 2.23688915], test AUC 0.9638089131796866\n",
      "epoch 0 iter 34815: train loss [0.17332469 6.17841394 2.23943583], test AUC 0.9621506690910152\n",
      "epoch 0 iter 36863: train loss [0.18981575 6.21082727 2.24867082], test AUC 0.971052139190147\n",
      "epoch 0 iter 38911: train loss [0.20517431 6.27481912 2.25165435], test AUC 0.9707944736751959\n",
      "epoch 1 iter 2047: train loss [0.17477323 6.46533845 2.26246785], test AUC 0.9743244093447968\n",
      "epoch 1 iter 4095: train loss [0.18230336 6.67637823 2.25237928], test AUC 0.9710977219666267\n",
      "epoch 1 iter 6143: train loss [0.17296588 6.96682859 2.29349763], test AUC 0.9756396225691517\n",
      "epoch 1 iter 8191: train loss [0.18333701 7.19187786 2.28167863], test AUC 0.9774354338395342\n",
      "epoch 1 iter 10239: train loss [0.15264082 7.30870837 2.28200263], test AUC 0.9780854660681949\n",
      "epoch 1 iter 12287: train loss [0.16549265 7.3897199  2.28642862], test AUC 0.9664732537599992\n",
      "epoch 1 iter 14335: train loss [0.15088239 7.49077296 2.27424073], test AUC 0.979378297330446\n",
      "epoch 1 iter 16383: train loss [0.16230273 7.65485064 2.27720505], test AUC 0.978112556430863\n",
      "epoch 1 iter 18431: train loss [0.14344694 7.7805361  2.28919218], test AUC 0.9763307338867956\n",
      "epoch 1 iter 20479: train loss [0.15084116 7.78592151 2.22904465], test AUC 0.9736996932988814\n",
      "epoch 1 iter 22527: train loss [0.15255367 7.88269137 2.23885316], test AUC 0.9799209234360844\n",
      "epoch 1 iter 24575: train loss [0.14755508 7.90453878 2.21784955], test AUC 0.9806277294222765\n",
      "epoch 1 iter 26623: train loss [0.15401176 7.90441275 2.21497829], test AUC 0.9800919270855725\n",
      "epoch 1 iter 28671: train loss [0.14675609 7.98314618 2.21567485], test AUC 0.9793591907774307\n",
      "epoch 1 iter 30719: train loss [0.12955686 7.99034259 2.19048971], test AUC 0.9792416172386964\n",
      "epoch 1 iter 32767: train loss [0.15595303 7.9791945  2.1804573 ], test AUC 0.9778140847776864\n",
      "epoch 1 iter 34815: train loss [0.13221118 8.00222641 2.17676948], test AUC 0.9836742693005868\n",
      "epoch 1 iter 36863: train loss [0.14003824 8.03567602 2.19481282], test AUC 0.9805592870198676\n",
      "epoch 1 iter 38911: train loss [0.13545218 8.06053511 2.1615542 ], test AUC 0.983008542403378\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "best_auc = 0\n",
    "topicModel.train()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = np.zeros(3)\n",
    "    skip = 0\n",
    "    for i, (input_ids,attention_mask, label, score, topic) in enumerate(train_loader):\n",
    "        # train\n",
    "        input_ids,attention_mask, label, score, topic = input_ids.to('cuda'),attention_mask.to('cuda'), label.to('cuda'), score.to('cuda'), topic.to('cuda')\n",
    "        loss,loss_tuple = model_lm.get_loss(input_ids,attention_mask, label, score,topic)\n",
    "        if math.isinf(loss.item()) or math.isnan(loss.item()):\n",
    "            skip += 1\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += np.array(loss_tuple)\n",
    "        # print(i,train_loss)\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # eval    \n",
    "        if (i + 1) % verbose == 0:\n",
    "            model.eval()\n",
    "            train_loss /= (verbose-skip)\n",
    "            yhat = []\n",
    "            for input_ids,attention_mask in val_loader:\n",
    "                input_ids,attention_mask = input_ids.to('cuda'),attention_mask.to('cuda')\n",
    "                out = model_lm.predict(input_ids,attention_mask).detach().cpu().numpy()\n",
    "                yhat.append(out)\n",
    "            yhat = np.concatenate(yhat)\n",
    "            auc = roc_auc_score(val.label.to_numpy(), yhat)\n",
    "            print(f\"epoch {epoch} iter {i}: train loss {train_loss}, test AUC {auc}\")\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                torch.save({k: v for k, v in model.state_dict().items() if k in trainable_names}, save_model_path+'/weights.pth')\n",
    "            train_loss = np.zeros(3)\n",
    "            skip = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save config and local score\n",
    "config = save_config(TARGET_MODEL, pred_type, config_type, epochs, alpha, aug_kwargs, config_kwargs)\n",
    "config['local auc'] = best_auc\n",
    "with open(save_model_path+'/config.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(config, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModelForCausalLM\n",
    "# import torch.nn as nn\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import math\n",
    "# import pickle\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import nlpaug.augmenter.word as naw\n",
    "# from functools import partial\n",
    "# from functions import *\n",
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# batch_size = 1\n",
    "# num_workers = 16\n",
    "# accumulation_steps = 64\n",
    "# verbose = 2048\n",
    "# lr = 8e-5\n",
    "# clip = 8e-3\n",
    "\n",
    "# save_model_path = 'Model/model03'\n",
    "# device = 'cuda'\n",
    "# # load hyper-parameter for inference\n",
    "# with open(save_model_path+'/config.pkl', 'rb') as pickle_file:\n",
    "#     config = pickle.load(pickle_file)\n",
    "# TARGET_MODEL, pred_type, config_type, epochs, alpha, aug_kwargs, config_class, config_kwargs = load_config(config)\n",
    "\n",
    "# val = pd.read_csv('data/val_data.csv')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# if pred_type == 'LM':\n",
    "#     prompt = tokenizer.batch_encode_plus(['Is this essay AI-generated, yes or no?'],add_special_tokens=False,return_tensors='pt')\n",
    "#     prompt,prompt_mask = prompt['input_ids'],prompt['attention_mask']\n",
    "# else:\n",
    "#     prompt,prompt_mask = None, None\n",
    "# val_data = InfData(val)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size*4, shuffle=False, num_workers=num_workers, \\\n",
    "#                         collate_fn=partial(collate_inf,tokenizer=tokenizer,prompt=prompt,prompt_mask=prompt_mask))\n",
    "# # input_ids,attention_mask, label, score = next(iter(train_loader))\n",
    "\n",
    "# from torch.nn.utils import clip_grad_value_\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# from peft import get_peft_model\n",
    "\n",
    "# nf4_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    bnb_4bit_quant_type=\"nf4\",\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "# base_class = AutoModelForCausalLM if pred_type == 'LM' else AutoModelForSequenceClassification\n",
    "# base_model = base_class.from_pretrained(TARGET_MODEL,quantization_config=nf4_config, \\\n",
    "#                                                           device_map={\"\":0},use_flash_attention_2=True)\n",
    "# peft_config = config_class(**config_kwargs)    \n",
    "# model = get_peft_model(base_model, peft_config)\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# model.load_state_dict(torch.load(save_model_path+'/weights.pth'),strict=False)\n",
    "# model = model.half()\n",
    "# if pred_type == 'LM':\n",
    "#     model_lm = LM(model,tokenizer,config_kwargs.get('num_virtual_tokens', 0),alpha,config_type)\n",
    "# else:\n",
    "#     model_lm = Classification(model,alpha)\n",
    "\n",
    "# yhat = []\n",
    "# for input_ids,attention_mask in val_loader:\n",
    "#     input_ids,attention_mask = input_ids.to('cuda'),attention_mask.to('cuda')\n",
    "#     out = model_lm.predict(input_ids,attention_mask).detach().cpu().numpy()\n",
    "#     yhat.append(out)\n",
    "# yhat = np.concatenate(yhat)\n",
    "# auc = roc_auc_score(val.label.to_numpy(), yhat)\n",
    "# print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
