{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MistralForSequenceClassification\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import nlpaug.augmenter.word as naw\n",
    "from functools import partial\n",
    "from functions import *\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 16\n",
    "TARGET_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "save_model_path = 'Model/p_tune'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('data/val_data.csv')\n",
    "train = pd.read_csv('data/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://paraphrase.org/#/download\n",
    "# https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb\n",
    "aug = naw.SynonymAug(aug_src='ppdb',model_path='Model/ppdb-2.0-s-all',\n",
    "                     aug_min=1,\n",
    "                     aug_max=10,\n",
    "                     aug_p=0.3)\n",
    "# print(aug.augment(train.text.iloc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TxtData(train,aug.augment)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, \\\n",
    "                          collate_fn=partial(collate_fn,tokenizer=tokenizer))\n",
    "val_data = TxtData(val)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size*2, shuffle=False, num_workers=num_workers, \\\n",
    "                          collate_fn=partial(collate_fn,tokenizer=tokenizer))\n",
    "# input_ids,attention_mask, label, score = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "verbose = 2048\n",
    "lr = 6e-5\n",
    "clip = 6e-3\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_value_\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PeftType,\n",
    "    PromptEncoderConfig)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4490c508224dfcbc8e60f467d98af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = MistralForSequenceClassification.from_pretrained(TARGET_MODEL,num_labels=2,quantization_config=nf4_config, \\\n",
    "                                                          device_map={\"\":0},use_flash_attention_2=True)\n",
    "peft_type = PeftType.P_TUNING\n",
    "peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=8, \\\n",
    "                                  encoder_hidden_size=4096,encoder_dropout=0.1,\\\n",
    "                                  encoder_num_layers=2,encoder_reparameterization_type='MLP')#'LSTM')\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.print_trainable_parameters()\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "# optimizer = torch.optim.AdamW(trainable_params,lr = lr,amsgrad=True,weight_decay=6e-3)\n",
    "optimizer = torch.optim.SGD(trainable_params,lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "best_auc = 0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    skip = 0\n",
    "    for i, (input_ids,attention_mask, label, score) in enumerate(train_loader):\n",
    "        # train\n",
    "        input_ids,attention_mask, label, score = input_ids.to('cuda'),attention_mask.to('cuda'), label.to('cuda'), score.to('cuda')\n",
    "        out = model(input_ids=input_ids,attention_mask=attention_mask).logits\n",
    "        if torch.any(torch.isnan(out)):\n",
    "            skip += 1\n",
    "            continue\n",
    "        loss_label = loss_fn(out[:,0], label)\n",
    "        loss_score = torch.sum((score!=-1.)*torch.abs(out[:,1]-score))/torch.sum((score!=-1.)+0.01)\n",
    "        loss = loss_label + loss_score * alpha\n",
    "        if math.isinf(loss.item()) or math.isnan(loss.item()):\n",
    "            skip += 1\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        # print(i,train_loss)\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # eval    \n",
    "        if (i + 1) % verbose == 0:\n",
    "            model.eval()\n",
    "            train_loss /= (verbose-skip)\n",
    "            yhat,y = [],[]\n",
    "            for input_ids,attention_mask, label, score in val_loader:\n",
    "                input_ids,attention_mask = input_ids.to('cuda'),attention_mask.to('cuda')\n",
    "                with torch.no_grad():\n",
    "                    out = model(input_ids=input_ids,attention_mask=attention_mask).logits[:,0].detach().cpu().numpy()\n",
    "                if np.any(np.isnan(out)):\n",
    "                    continue\n",
    "                yhat.append(out)\n",
    "                y.append(label)\n",
    "            yhat = np.concatenate(yhat)\n",
    "            y = np.concatenate(y)\n",
    "            auc = roc_auc_score(y, yhat)\n",
    "            print(f\"epoch {epoch} iter {i}: train loss {train_loss}, test AUC {auc}\")\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                torch.save({k: v for k, v in model.state_dict().items() if k in trainable_params}, save_model_path+'/weights.pth')\n",
    "            train_loss = 0\n",
    "            skip = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.embed_tokens.weight', 'base_model.model.layers.0.self_attn.q_proj.weight', 'base_model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.layers.0.self_attn.v_proj.weight', 'base_model.model.layers.0.self_attn.o_proj.weight', 'base_model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.layers.0.mlp.up_proj.weight', 'base_model.model.layers.0.mlp.down_proj.weight', 'base_model.model.layers.0.input_layernorm.weight', 'base_model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.layers.1.self_attn.q_proj.weight', 'base_model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.layers.1.self_attn.v_proj.weight', 'base_model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.layers.1.mlp.up_proj.weight', 'base_model.model.layers.1.mlp.down_proj.weight', 'base_model.model.layers.1.input_layernorm.weight', 'base_model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.layers.2.self_attn.q_proj.weight', 'base_model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.layers.2.self_attn.v_proj.weight', 'base_model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.layers.2.mlp.up_proj.weight', 'base_model.model.layers.2.mlp.down_proj.weight', 'base_model.model.layers.2.input_layernorm.weight', 'base_model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.layers.3.self_attn.q_proj.weight', 'base_model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.layers.3.self_attn.v_proj.weight', 'base_model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.layers.3.mlp.up_proj.weight', 'base_model.model.layers.3.mlp.down_proj.weight', 'base_model.model.layers.3.input_layernorm.weight', 'base_model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.layers.4.self_attn.q_proj.weight', 'base_model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.layers.4.self_attn.v_proj.weight', 'base_model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.layers.4.mlp.up_proj.weight', 'base_model.model.layers.4.mlp.down_proj.weight', 'base_model.model.layers.4.input_layernorm.weight', 'base_model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.layers.5.self_attn.q_proj.weight', 'base_model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.layers.5.self_attn.v_proj.weight', 'base_model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.layers.5.mlp.up_proj.weight', 'base_model.model.layers.5.mlp.down_proj.weight', 'base_model.model.layers.5.input_layernorm.weight', 'base_model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.layers.6.self_attn.q_proj.weight', 'base_model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.layers.6.self_attn.v_proj.weight', 'base_model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.layers.6.mlp.up_proj.weight', 'base_model.model.layers.6.mlp.down_proj.weight', 'base_model.model.layers.6.input_layernorm.weight', 'base_model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.layers.7.self_attn.q_proj.weight', 'base_model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.layers.7.self_attn.v_proj.weight', 'base_model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.layers.7.mlp.up_proj.weight', 'base_model.model.layers.7.mlp.down_proj.weight', 'base_model.model.layers.7.input_layernorm.weight', 'base_model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.layers.8.self_attn.q_proj.weight', 'base_model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.layers.8.self_attn.v_proj.weight', 'base_model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.layers.8.mlp.up_proj.weight', 'base_model.model.layers.8.mlp.down_proj.weight', 'base_model.model.layers.8.input_layernorm.weight', 'base_model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.layers.9.self_attn.q_proj.weight', 'base_model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.layers.9.self_attn.v_proj.weight', 'base_model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.layers.9.mlp.up_proj.weight', 'base_model.model.layers.9.mlp.down_proj.weight', 'base_model.model.layers.9.input_layernorm.weight', 'base_model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.layers.10.self_attn.q_proj.weight', 'base_model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.layers.10.self_attn.v_proj.weight', 'base_model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.layers.10.mlp.up_proj.weight', 'base_model.model.layers.10.mlp.down_proj.weight', 'base_model.model.layers.10.input_layernorm.weight', 'base_model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.layers.11.self_attn.q_proj.weight', 'base_model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.layers.11.self_attn.v_proj.weight', 'base_model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.layers.11.mlp.up_proj.weight', 'base_model.model.layers.11.mlp.down_proj.weight', 'base_model.model.layers.11.input_layernorm.weight', 'base_model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.layers.12.self_attn.q_proj.weight', 'base_model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.layers.12.self_attn.v_proj.weight', 'base_model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.layers.12.mlp.up_proj.weight', 'base_model.model.layers.12.mlp.down_proj.weight', 'base_model.model.layers.12.input_layernorm.weight', 'base_model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.layers.13.self_attn.q_proj.weight', 'base_model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.layers.13.self_attn.v_proj.weight', 'base_model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.layers.13.mlp.up_proj.weight', 'base_model.model.layers.13.mlp.down_proj.weight', 'base_model.model.layers.13.input_layernorm.weight', 'base_model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.layers.14.self_attn.q_proj.weight', 'base_model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.layers.14.self_attn.v_proj.weight', 'base_model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.layers.14.mlp.up_proj.weight', 'base_model.model.layers.14.mlp.down_proj.weight', 'base_model.model.layers.14.input_layernorm.weight', 'base_model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.layers.15.self_attn.q_proj.weight', 'base_model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.layers.15.self_attn.v_proj.weight', 'base_model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.layers.15.mlp.up_proj.weight', 'base_model.model.layers.15.mlp.down_proj.weight', 'base_model.model.layers.15.input_layernorm.weight', 'base_model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.layers.16.self_attn.q_proj.weight', 'base_model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.layers.16.self_attn.v_proj.weight', 'base_model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.layers.16.mlp.up_proj.weight', 'base_model.model.layers.16.mlp.down_proj.weight', 'base_model.model.layers.16.input_layernorm.weight', 'base_model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.layers.17.self_attn.q_proj.weight', 'base_model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.layers.17.self_attn.v_proj.weight', 'base_model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.layers.17.mlp.up_proj.weight', 'base_model.model.layers.17.mlp.down_proj.weight', 'base_model.model.layers.17.input_layernorm.weight', 'base_model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.layers.18.self_attn.q_proj.weight', 'base_model.model.layers.18.self_attn.k_proj.weight', 'base_model.model.layers.18.self_attn.v_proj.weight', 'base_model.model.layers.18.self_attn.o_proj.weight', 'base_model.model.layers.18.mlp.gate_proj.weight', 'base_model.model.layers.18.mlp.up_proj.weight', 'base_model.model.layers.18.mlp.down_proj.weight', 'base_model.model.layers.18.input_layernorm.weight', 'base_model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.layers.19.self_attn.q_proj.weight', 'base_model.model.layers.19.self_attn.k_proj.weight', 'base_model.model.layers.19.self_attn.v_proj.weight', 'base_model.model.layers.19.self_attn.o_proj.weight', 'base_model.model.layers.19.mlp.gate_proj.weight', 'base_model.model.layers.19.mlp.up_proj.weight', 'base_model.model.layers.19.mlp.down_proj.weight', 'base_model.model.layers.19.input_layernorm.weight', 'base_model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.layers.20.self_attn.q_proj.weight', 'base_model.model.layers.20.self_attn.k_proj.weight', 'base_model.model.layers.20.self_attn.v_proj.weight', 'base_model.model.layers.20.self_attn.o_proj.weight', 'base_model.model.layers.20.mlp.gate_proj.weight', 'base_model.model.layers.20.mlp.up_proj.weight', 'base_model.model.layers.20.mlp.down_proj.weight', 'base_model.model.layers.20.input_layernorm.weight', 'base_model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.layers.21.self_attn.q_proj.weight', 'base_model.model.layers.21.self_attn.k_proj.weight', 'base_model.model.layers.21.self_attn.v_proj.weight', 'base_model.model.layers.21.self_attn.o_proj.weight', 'base_model.model.layers.21.mlp.gate_proj.weight', 'base_model.model.layers.21.mlp.up_proj.weight', 'base_model.model.layers.21.mlp.down_proj.weight', 'base_model.model.layers.21.input_layernorm.weight', 'base_model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.layers.22.self_attn.q_proj.weight', 'base_model.model.layers.22.self_attn.k_proj.weight', 'base_model.model.layers.22.self_attn.v_proj.weight', 'base_model.model.layers.22.self_attn.o_proj.weight', 'base_model.model.layers.22.mlp.gate_proj.weight', 'base_model.model.layers.22.mlp.up_proj.weight', 'base_model.model.layers.22.mlp.down_proj.weight', 'base_model.model.layers.22.input_layernorm.weight', 'base_model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.layers.23.self_attn.q_proj.weight', 'base_model.model.layers.23.self_attn.k_proj.weight', 'base_model.model.layers.23.self_attn.v_proj.weight', 'base_model.model.layers.23.self_attn.o_proj.weight', 'base_model.model.layers.23.mlp.gate_proj.weight', 'base_model.model.layers.23.mlp.up_proj.weight', 'base_model.model.layers.23.mlp.down_proj.weight', 'base_model.model.layers.23.input_layernorm.weight', 'base_model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.layers.24.self_attn.q_proj.weight', 'base_model.model.layers.24.self_attn.k_proj.weight', 'base_model.model.layers.24.self_attn.v_proj.weight', 'base_model.model.layers.24.self_attn.o_proj.weight', 'base_model.model.layers.24.mlp.gate_proj.weight', 'base_model.model.layers.24.mlp.up_proj.weight', 'base_model.model.layers.24.mlp.down_proj.weight', 'base_model.model.layers.24.input_layernorm.weight', 'base_model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.layers.25.self_attn.q_proj.weight', 'base_model.model.layers.25.self_attn.k_proj.weight', 'base_model.model.layers.25.self_attn.v_proj.weight', 'base_model.model.layers.25.self_attn.o_proj.weight', 'base_model.model.layers.25.mlp.gate_proj.weight', 'base_model.model.layers.25.mlp.up_proj.weight', 'base_model.model.layers.25.mlp.down_proj.weight', 'base_model.model.layers.25.input_layernorm.weight', 'base_model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.layers.26.self_attn.q_proj.weight', 'base_model.model.layers.26.self_attn.k_proj.weight', 'base_model.model.layers.26.self_attn.v_proj.weight', 'base_model.model.layers.26.self_attn.o_proj.weight', 'base_model.model.layers.26.mlp.gate_proj.weight', 'base_model.model.layers.26.mlp.up_proj.weight', 'base_model.model.layers.26.mlp.down_proj.weight', 'base_model.model.layers.26.input_layernorm.weight', 'base_model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.layers.27.self_attn.q_proj.weight', 'base_model.model.layers.27.self_attn.k_proj.weight', 'base_model.model.layers.27.self_attn.v_proj.weight', 'base_model.model.layers.27.self_attn.o_proj.weight', 'base_model.model.layers.27.mlp.gate_proj.weight', 'base_model.model.layers.27.mlp.up_proj.weight', 'base_model.model.layers.27.mlp.down_proj.weight', 'base_model.model.layers.27.input_layernorm.weight', 'base_model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.layers.28.self_attn.q_proj.weight', 'base_model.model.layers.28.self_attn.k_proj.weight', 'base_model.model.layers.28.self_attn.v_proj.weight', 'base_model.model.layers.28.self_attn.o_proj.weight', 'base_model.model.layers.28.mlp.gate_proj.weight', 'base_model.model.layers.28.mlp.up_proj.weight', 'base_model.model.layers.28.mlp.down_proj.weight', 'base_model.model.layers.28.input_layernorm.weight', 'base_model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.layers.29.self_attn.q_proj.weight', 'base_model.model.layers.29.self_attn.k_proj.weight', 'base_model.model.layers.29.self_attn.v_proj.weight', 'base_model.model.layers.29.self_attn.o_proj.weight', 'base_model.model.layers.29.mlp.gate_proj.weight', 'base_model.model.layers.29.mlp.up_proj.weight', 'base_model.model.layers.29.mlp.down_proj.weight', 'base_model.model.layers.29.input_layernorm.weight', 'base_model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.layers.30.self_attn.q_proj.weight', 'base_model.model.layers.30.self_attn.k_proj.weight', 'base_model.model.layers.30.self_attn.v_proj.weight', 'base_model.model.layers.30.self_attn.o_proj.weight', 'base_model.model.layers.30.mlp.gate_proj.weight', 'base_model.model.layers.30.mlp.up_proj.weight', 'base_model.model.layers.30.mlp.down_proj.weight', 'base_model.model.layers.30.input_layernorm.weight', 'base_model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.layers.31.self_attn.q_proj.weight', 'base_model.model.layers.31.self_attn.k_proj.weight', 'base_model.model.layers.31.self_attn.v_proj.weight', 'base_model.model.layers.31.self_attn.o_proj.weight', 'base_model.model.layers.31.mlp.gate_proj.weight', 'base_model.model.layers.31.mlp.up_proj.weight', 'base_model.model.layers.31.mlp.down_proj.weight', 'base_model.model.layers.31.input_layernorm.weight', 'base_model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.norm.weight', 'base_model.score.original_module.weight', 'word_embeddings.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only save/load the trainable.\n",
    "# model.load_state_dict(torch.load(save_model_path+'/weights2.pth'),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.base_model.save_pretrained(save_model_path,safe_serialization=False)\n",
    "# model.config.save_pretrained(save_model_path)\n",
    "# model.save_pretrained(save_model_path)\n",
    "# tokenizer.save_pretrained(save_model_path)\n",
    "\n",
    "\n",
    "# from peft import PeftModel\n",
    "# baseModel = MistralForSequenceClassification.from_pretrained(TARGET_MODEL,num_labels=2,quantization_config=nf4_config, \\\n",
    "#                                                           device_map={\"\":0},use_flash_attention_2=True)\n",
    "# peft_type = PeftType.P_TUNING\n",
    "# peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=8, \\\n",
    "#                                   encoder_hidden_size=4096,encoder_dropout=0.1,\\\n",
    "#                                   encoder_num_layers=2,encoder_reparameterization_type='MLP')#'LSTM')\n",
    "# model2 = PeftModel.from_pretrained(baseModel, 'Model/p_tune/adapter',config=peft_config)\n",
    "\n",
    "\n",
    "# import json\n",
    "# with open('Model/p_tune/config.json', 'r') as file:\n",
    "#     config = json.load(file)\n",
    "# from transformers import PretrainedConfig\n",
    "# model = MistralForSequenceClassification(PretrainedConfig(**config))\n",
    "# peft_type = PeftType.P_TUNING\n",
    "# peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=8, \\\n",
    "#                                   encoder_hidden_size=4096,encoder_dropout=0.1,\\\n",
    "#                                   encoder_num_layers=2,encoder_reparameterization_type='MLP')#'LSTM')\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.load_state_dict(torch.load('Model/p_tune.pth'))\n",
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# yhat,y = [],[]\n",
    "# for input_ids,attention_mask, label, score in val_loader:\n",
    "#     input_ids,attention_mask = input_ids.to('cuda'),attention_mask.to('cuda')\n",
    "#     with torch.no_grad():\n",
    "#         out = model(input_ids=input_ids,attention_mask=attention_mask).logits[:,0].detach().cpu().numpy()\n",
    "#     if np.any(np.isnan(out)):\n",
    "#         continue\n",
    "#     yhat.append(out)\n",
    "#     y.append(label)\n",
    "# yhat = np.concatenate(yhat)\n",
    "# y = np.concatenate(y)\n",
    "# auc = roc_auc_score(y, yhat)\n",
    "# auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
